{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluating and Debugging Generative AI"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "During the iterative workflow of build a Machine Learning model, many time we come across with the fact thas a previous iteration was actually better and we will like to return to the previous assemble of hyperparameters and training set. Managining a proper and clean way to work around this can be challenging even for a small team of developers. In order to properly do this, it is important to be rigurous in the ways that this is managed.\n",
    "\n",
    "This course spins around the tools from **Weights & Biases**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Instrument W&B\n",
    "\n",
    "First, not mentioned in the course, *W&B* offers a bunch of levels on their service. The most important one here, the personal development level is free. You can also build your own server of *W&B* or execute the code anonymously without an account"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# %pip install wandb\n",
    "import wandb\n",
    "\n",
    "# 1. Organize your hyperparameters\n",
    "config = {'learning_rate': 0.001}\n",
    "\n",
    "# 2. Start wandb run\n",
    "wandb.init(project='gpt5, config=config)\n",
    "\n",
    "#--- Model training ---#\n",
    "\n",
    "# 3. Log metrics over time to visualize performance\n",
    "wandb.log({'loss': loss})\n",
    "\n",
    "# 4. When working in a notebook, finish wandb\n",
    "wandb.finish()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The idea here is to implement *Wandb* directly inside the functions used to train me model in order to automatically log every step made in the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example\n",
    "\n",
    "The example is to classify 4 sprites (small pixelart images) using neural-networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "INPUT_SIZE = 3 * 16 * 16\n",
    "OUTPUT_SIZE = 5\n",
    "HIDDEN_SIZE = 256\n",
    "NUM_WORKERS = 2\n",
    "CLASSES = [\"hero\", \"non-hero\", \"food\", \"spell\", \"side-facing\"]\n",
    "DATA_DIR = Path('./data/')\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available()  else \"cpu\")\n",
    "\n",
    "def get_model(dropout):\n",
    "    \"Simple MLP with Dropout\"\n",
    "    return nn.Sequential(\n",
    "        nn.Flatten(),\n",
    "        nn.Linear(INPUT_SIZE, HIDDEN_SIZE),\n",
    "        nn.BatchNorm1d(HIDDEN_SIZE),\n",
    "        nn.ReLU(),\n",
    "        nn.Dropout(dropout),\n",
    "        nn.Linear(HIDDEN_SIZE, OUTPUT_SIZE)\n",
    "    ).to(DEVICE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we define a python object (a dictionary) with the hyperparameters that we will use to train our model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Let's define a config object to store our hyperparameters\n",
    "config = SimpleNamespace(\n",
    "    epochs = 2,\n",
    "    batch_size = 128,\n",
    "    lr = 1e-5,\n",
    "    dropout = 0.5,\n",
    "    slice_size = 10_000,\n",
    "    valid_pct = 0.2,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Until now, everything is standard to machine learning algorithms.\n",
    "\n",
    "In the standard training fuction, we have: the loading of the training data set and its slice into training and validation sets, the definition of the steps per epoch (though we will probably has as well an early stop), chosing the loss function and the optimizer, and finally the loop to train the model per epoch.\n",
    "\n",
    "It is in this function where we will intertwine the wandb code "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "def train_model(config):\n",
    "    \"Train a model with a given config\"\n",
    "    \n",
    "    wandb.init(\n",
    "        project=\"dlai_intro\",\n",
    "        config=config,\n",
    "    )\n",
    "\n",
    "    # Get the data\n",
    "    train_dl, valid_dl = get_dataloaders(DATA_DIR, \n",
    "                                         config.batch_size, \n",
    "                                         config.slice_size, \n",
    "                                         config.valid_pct)\n",
    "    n_steps_per_epoch = math.ceil(len(train_dl.dataset) / config.batch_size)\n",
    "\n",
    "    # A simple MLP model\n",
    "    model = get_model(config.dropout)\n",
    "\n",
    "    # Make the loss and optimizer\n",
    "    loss_func = nn.CrossEntropyLoss()\n",
    "    optimizer = Adam(model.parameters(), lr=config.lr)\n",
    "\n",
    "    example_ct = 0\n",
    "\n",
    "    for epoch in tqdm(range(config.epochs), total=config.epochs):\n",
    "        model.train()\n",
    "\n",
    "        for step, (images, labels) in enumerate(train_dl):\n",
    "            images, labels = images.to(DEVICE), labels.to(DEVICE)\n",
    "\n",
    "            outputs = model(images)\n",
    "            train_loss = loss_func(outputs, labels)\n",
    "            optimizer.zero_grad()\n",
    "            train_loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            example_ct += len(images)\n",
    "            metrics = {\n",
    "                \"train/train_loss\": train_loss,\n",
    "                \"train/epoch\": epoch + 1,\n",
    "                \"train/example_ct\": example_ct\n",
    "            }\n",
    "            wandb.log(metrics)\n",
    "            \n",
    "        # Compute validation metrics, log images on last epoch\n",
    "        val_loss, accuracy = validate_model(model, valid_dl, loss_func)\n",
    "        # Compute train and validation metrics\n",
    "        val_metrics = {\n",
    "            \"val/val_loss\": val_loss,\n",
    "            \"val/val_accuracy\": accuracy\n",
    "        }\n",
    "        wandb.log(val_metrics)\n",
    "    \n",
    "    wandb.finish()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When executing the model using *W&B*, we will receive some links to the iteration and the project. This will lead us to a dashboard were we can explore the graph of the experiment that we carried out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
